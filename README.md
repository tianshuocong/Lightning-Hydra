# Lightning-Hydra

ðŸš¡ Let's learn how to use [lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template)!

## Introduction

- [PyTorch Lightning](https://github.com/Lightning-AI/lightning): a lightweight PyTorch wrapper for high-performance AI research. Think of it as a framework for organizing your PyTorch code.

- [Hydra](https://github.com/facebookresearch/hydra): a framework for elegantly configuring complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line.


## Project Structure

```
â”œâ”€â”€ configs                   <- Hydra configuration files
â”‚   â”œâ”€â”€ callbacks                <- Callbacks configs
â”‚   â”œâ”€â”€ datamodule               <- Datamodule configs
â”‚   â”œâ”€â”€ debug                    <- Debugging configs
â”‚   â”œâ”€â”€ experiment               <- Experiment configs
â”‚   â”œâ”€â”€ extras                   <- Extra utilities configs
â”‚   â”œâ”€â”€ hparams_search           <- Hyperparameter search configs
â”‚   â”œâ”€â”€ hydra                    <- Hydra configs
â”‚   â”œâ”€â”€ local                    <- Local configs
â”‚   â”œâ”€â”€ logger                   <- Logger configs
â”‚   â”œâ”€â”€ model                    <- Model configs
â”‚   â”œâ”€â”€ paths                    <- Project paths configs
â”‚   â”œâ”€â”€ trainer                  <- Trainer configs
â”‚   â”‚
â”‚   â”œâ”€â”€ eval.yaml             <- Main config for evaluation
â”‚   â””â”€â”€ train.yaml            <- Main config for training
â”‚
â”œâ”€â”€ data                   <- Project data
â”‚
â”œâ”€â”€ logs                   <- Logs generated by hydra and lightning loggers
â”‚
â”œâ”€â”€ notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
â”‚                             the creator's initials, and a short `-` delimited description,
â”‚                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
â”‚
â”œâ”€â”€ scripts                <- Shell scripts
â”‚
â”œâ”€â”€ src                    <- Source code
â”‚   â”œâ”€â”€ datamodules              <- Lightning datamodules
â”‚   â”œâ”€â”€ models                   <- Lightning models
â”‚   â”œâ”€â”€ utils                    <- Utility scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ eval.py                  <- Run evaluation
â”‚   â””â”€â”€ train.py                 <- Run training
â”‚
â”œâ”€â”€ tests                  <- Tests of any kind
â”‚
â”œâ”€â”€ .env.example              <- Example of file for storing private environment variables
â”œâ”€â”€ .gitignore                <- List of files ignored by git
â”œâ”€â”€ .pre-commit-config.yaml   <- Configuration of pre-commit hooks for code formatting
â”œâ”€â”€ Makefile                  <- Makefile with commands like `make train` or `make test`
â”œâ”€â”€ pyproject.toml            <- Configuration options for testing and linting
â”œâ”€â”€ requirements.txt          <- File for installing python dependencies
â”œâ”€â”€ setup.py                  <- File for installing project as a package
â””â”€â”€ README.md
```

## Command Line

- Config parameter
`python train.py trainer.max_epochs=20 model.optimizer.lr=1e-4`

- Add new parameters `python train.py +model.new_param="owo"`

- Training device `python train.py trainer=gpu`

- Train with mixed precision `python train.py trainer=gpu +trainer.precision=16`

- Train model with [configs/experiment/example.yaml](https://github.com/ashleve/lightning-hydra-template/blob/main/configs/experiment/example.yaml) `python train.py experiment=example`

- Resume ckpt `python train.py ckpt_path="/path/to/ckpt/name.ckpt"`

- Evaluate ckpt on test dataset `python eval.py ckpt_path="/path/to/ckpt/name.ckpt"`

- Create a sweep over hyperparameters `python train.py -m datamodule.batch_size=32,64,128 model.lr=0.001,0.0005` 

  ðŸ¥‘ result in 6 different combination 

- :heart: HPO with Optuna: `python train.py -m hparams_search=mnist_optuna experiment=example`  We can define everything in a single [config file](https://github.com/ashleve/lightning-hydra-template/blob/main/configs/hparams_search/mnist_optuna.yaml)

- Execute all experiments from the folder `configs/experiment/` through `python train.py -m 'experiment=glob(*)'`

- Execute with multiple seed `python train.py -m seed=1,2,3,4,5 trainer.deterministic=True logger=csv tags=["benchmark"]`

## Workflow

1. Write PyTorch Lightning module `models/mnist_module.py`

2. 





- Model config:
```yaml
_target_: src.models.mnist_model.MNISTLitModule
lr: 0.001
net:
  _target_: src.models.components.simple_dense_net.SimpleDenseNet
  input_size: 784
  lin1_size: 256
  lin2_size: 256
  lin3_size: 256
  output_size: 10
```

- instantiate the object
```python
model = hydra.utils.instantiate(config.model)
```
- command line
```bash
python train.py model=mnist
```

## Q&A

- [Conditional Search space](https://github.com/facebookresearch/hydra/issues/1906)
